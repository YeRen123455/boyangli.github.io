<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
	<style>
		body{
			color: #333;
		}
		#container{
			min-width: 1000px;
			width: 1000px;
/*			overflow: auto;
*/			margin: 50px auto;padding: 30px;
			/*zoom: 1;*/
*//*			border: 1px solid #ccc;background: #fc9;color: #fff;
*/		}
		#left{
			float: left;
			width: 300px;
			height: 200px;
			margin-left: 0px;
		}
		#right{
			float: left;
			width: auto;
			margin-left: 50px;
		}
		#name{
			font-size: 22.0pt;
		    mso-bidi-font-size: 24.0pt;
		    font-family: Times;
		    mso-bidi-font-family: Times;
		        font-weight: bold;
		}
		#info{
		    font-size: 16.0pt;
		    mso-bidi-font-size: 17.0pt;
		    font-family: Times;
		    mso-bidi-font-family: Times;
		    margin-top: 30px;
		    margin-left: 5px;
		    margin-bottom: 10px;
		    
		}
		.clear{clear:both; height: 0; line-height: 0; font-size: 0}
		.Bio{
			font-size:16.0pt;
			mso-bidi-font-size:17.0pt;
			line-height:150%;
			font-family:Times;
			mso-bidi-font-family:Lato-Regular;
			text-align: justify;
		}
		span.SpellE {
		    mso-style-name: "";
		    mso-spl-e: yes;
		}
		span.Title{
			    font-size: 22.0pt;
			    mso-bidi-font-size: 17.0pt;
			    font-family: Times;
			    mso-bidi-font-family: Lato-Regular;
			    font: bold;
		}
		div.section{
			padding-top: 30px;
		}
		
		div.sub-left{
			float: left;
			width: 250px;
						
		}
		div.sub-left img{
			vertical-align: middle;
			horizontal-align: middle;
			margin-top: 10px;
		}
		
		div.sub-left span{
			height: 100%;
			display: inline-block;
			vertical-align: top;
						
		}
		div.sub-right{
			float: left;
			width: 650px;			
		}
		.paper{
			overflow: auto;
			zoom:1;
			border-top: 2px dashed #333;
			padding-bottom: 0px;
			min-height: 150px;

		}
		.paperTitle{
			font-size:14.0pt;
			mso-bidi-font-size:18.0pt;
			font-family:Times;
			mso-bidi-font-family:Times;
			margin-top: 10px;
			margin-bottom: 10px;
			font-weight: bold;
		}
		.paperName,.paperPub{
		    font-size: 12.0pt;
		    mso-bidi-font-size: 13.0pt;		    
		    font-family: Times;
		    mso-bidi-font-family: Times;
		    line-height:150%;
		}
		.link{
		    font-size: 12.0pt;
		    mso-bidi-font-size: 13.0pt;
		    font-family: Times;
		    mso-bidi-font-family: Times;
		    margin-top: 10px;
		    margin-bottom: 0px;
		}
		.special{
		    margin-top: 0in;
		    margin-bottom: 0in;
		    margin-left: -.9pt;
		    margin-bottom: .0001pt;
		    text-indent: .9pt;
		    mso-pagination: none;
		    tab-stops: 13.75in;
		    mso-layout-grid-align: none;
		    text-autospace: none;
		}
		.long div.sub-left, .long div.sub-right{
			height:315px;

		}
		.short div.sub-left, .short div.sub-right{
			height:150px;

		}
		div.sub-left,div.sub-right{
			height:200px;

		}
	</style>
</head>
<body>
	<div id="container">
		<div id="left">
			<img width="300" height="225" src="https://raw.githubusercontent.com/YingqianWang/homepage/master/imgs/photo.jpg" />
		</div>
		<div id="right">
			<div id="name">Yingqian Wang（王应谦）</div>
			<div id="info">

				Ph.D. Student<p>
				National University of Defense Technology (NUDT), China<p>
				Email: wangyingqian16@nudt.edu.cn<p>				
			</div>

			         <a href="https://scholar.google.com/citations?user=tBA4alMAAAAJ&hl=en" target="_blank" rel="nofollow">Google Scholar</a>  |
			         <a href="https://www.researchgate.net/profile/Yingqian_Wang3?ev=prf_highl" target="_blank" rel="nofollow"><span>Research Gate</span></a>  |
			         <a href="https://github.com/YingqianWang" target="_blank" rel="nofollow"><span>Github</span></a>  |
			         <a href="https://blog.csdn.net/weixin_38490884" target="_blank" rel="nofollow"><span>Blog</span></a>
			</div>

		<div class="clear"></div>
		<div class="section">
			<span class="Title"><b>Brief Bio</b></span><p>			
				<div class="Bio">
					I received my B.E. degree from Shandong University in 2016, and received my M.E. degree from NUDT in 2018.
					Currently, I'm working toward the Ph.D. degree with the College of Electronic Science and Technology, NUDT.
					My research interests mainly focus on low-level vision, particularly on
					<b style="mso-bidi-font-weight:normal">light field imaging</b> and <b style="mso-bidi-font-weight:normal">image super-resolution</b>.</span></p>
				</div>


	<div class="section">
		<span class="Title"><b>News</b></span><p>			
		<div class="paperPub"><b>
			2020.09 | Our paper "Light Field Image Super-Resolution Using Deformable Convolution" is updated on arXiv. <br>
			2020.08 | We have one paper on light field refocusing accepted by IET Image Processing.<br>
			2020.07 | Our paper "Deformable 3D Convolution for Video Super-Resolution" is accepted by IEEE SPL.<br>
			2020.07 | Our paper "Spatial-Angular Interaction for Light Field Image Super-Resolution" is accepted to <span style="color:red">ECCV 2020</span>.<br>
			2020.06 | Our paper "Learning Sparse Masks for Efficient Image Super-Resolution" is posted on arXiv. <br>			
			2020.04 | We have collected and released some useful recources on <a href="https://github.com/YingqianWang/Awesome-Stereo-Image-SR" target="_blank" rel="nofollow">Stereo Image SR</a>
	and <a href="https://github.com/YingqianWang/Awesome-LF-Image-SR" target="_blank" rel="nofollow">LF Image SR</a>.<br>
			2020.02 | Our paper "A Stereo Attention Module for Stereo Image Super-Resolution" is accepted by IEEE SPL.<br>			
			2019.12 | Our paper "DeOccNet: Learning to See Through Foreground Occlusions in Light Fields" is accepted to WACV 2020.<br>
			2019.03 | A large-scale dataset for stereo image super-resolution is available online at <a href="https://yingqianwang.github.io/Flickr1024" target="_blank" rel="nofollow">Flickr1024</a>. <br>
			2019.02 | Our paper "Learning Parallax Attention for Stereo Image Super-Resolution" is accepted to <span style="color:red">CVPR 2019</span>.<br>

		</b></div>
	</div>
	
	
	<div class="clear"></div>
	<div class="section">
	<span class="Title"><b>Publications --- 2020</b></span><p><p>


                <div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/SFEAFE.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Spatial-Angular Interaction for Light Field Image Super-Resolution
					</div>
					<div class="paperName">
						<b>Yingqian Wang</b>, Longguang Wang, Jungang Yang, Wei An, Jingyi Yu, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"><b>New!</b> </span> European Conference on Computer Vision (ECCV), 2020.<br> 
						<span style="color:blue">Our LF-InterNet achieves SOTA SR performance with a low computational cost.</span><br>
						| <a href="https://arxiv.org/pdf/1912.07849.pdf" target="_blank" rel="nofollow">Paper</a>						
						| <a href="https://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&mid=2247520164&idx=1&sn=beb7031162b5c173e66a77b01a828e5b&chksm=96f0fdf0a18774e6a9f85e316efd64d9990dca2213e353c2da373b0a6f4004717e9b092923fb&mpshare=1&srcid=07057miMA6psG3L6UHJZOYb1&sharer_sharetime=1593964266602&sharer_shareid=eadfa6ebb7f5bf94747b471d67269b5e&from=timeline&scene=2&subscene=1&clicktime=1593965476&enterid=1593965476&ascene=14&devicetype=Windows+10+x64&version=62090529&nettype=3gnet&abtest_cookie=AAACAA%3D%3D&lang=zh_CN&exportkey=AbY3Hk%2Bj7%2FZr1bhOoseHkrg%3D&pass_ticket=xxF82PuauoRDV3pp3gMzbUWag4Wn9ibURKETDRhWZzgqvAuhhLZpHUAsxPEkeaTP&wx_header=1&key=d197f1f59dba99c28d4c1b99b04b6bd1077e51e74b63c15b1c47bf892712247f8c43b777c67399686f47ac2805132c5cecb7dd7d8fcd8ede07661f7a2dc7733f6be335fc77d4b5033daad98428a3d3ef&uin=MTM2MDA1MjgzOA%3D%3D" target="_blank" rel="nofollow">Report (Chinese)</a>
						| <a href="https://wyqdatabase.s3-us-west-1.amazonaws.com/LF-InterNet.mp4" target="_blank" rel="nofollow">Presentation</a>
						| <a href="https://github.com/YingqianWang/LF-InterNet" target="_blank" rel="nofollow">Code</a>
						<iframe src="https://ghbtns.com/github-btn.html?user=YingqianWang&repo=LF-InterNet&type=star&count=true&size=small"
							frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
					</div>
				</div>
			</div>
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/LF-DFnet.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Light Field Image Super-Resolution Using Deformable Convolution
					</div>
					<div class="paperName">
						<b>Yingqian Wang</b>, Jungang Yang, Longguang Wang, Xinyi Ying, Tianhao Wu, Wei An, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"><b>New!</b> </span> Submitted to IEEE TIP after a major revision, arXiv, 2020.<br> 
						<span style="color:blue">Our LF-DFnet achieves better SR performance than LF-InterNet with a smaller model size.</span><br>
						| <a href="https://arxiv.org/pdf/2007.03535.pdf" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://github.com/YingqianWang/NUDT-Dataset" target="_blank" rel="nofollow">Dataset</a>
					</div>
				</div>
			</div>
		
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/D3Dnet.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Deformable 3D Convolution for Video Super-Resolution
					</div>
					<div class="paperName">
						Xinyi Ying, Longguang Wang, <b>Yingqian Wang</b>, Weidong Sheng, Wei An, Yulan Guo
					</div>
					<div class="paperPub">
						</span> IEEE Signal Processing Letters, vol. 27, pp. 1500-1504, 2020.<br> 
						<span style="color:blue">Powerful in motion-aware spatio-temporal modeling.</span><br>
						| <a href="https://arxiv.org/pdf/2004.02803.pdf" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247502648&idx=2&sn=17e7e6b7aa13fc6cd39a2104de094919&chksm=ec1c26c1db6bafd7d62cbf50f8530bd035bbcac90994de9802370cddfec24fd451fa012c8e85&mpshare=1&scene=1&srcid=0730qMSCTUETD2peq7Ai9mqj&sharer_sharetime=1596065203111&sharer_shareid=989b0bb833dbad1aaaaf36960593e33d&key=e15af338f75799eee85c83cac60bac15ac1e15e525ccfd752c3e15a9507a29048c2a839916476162b7cd6631f963b446002008fb9da77ec44e2d36984d0c673fa2f3cbb751941bd96ca9c48496b49e0c&ascene=1&uin=MTM2MDA1MjgzOA%3D%3D&devicetype=Windows+10+x64&version=62090529&lang=zh_CN&exportkey=AVTx8nleIvzQ0rnSOvP%2Ftzg%3D&pass_ticket=%2F9wOS6SK9msDx7isZOq2Wr5mNTqm2vAhUeKvKok%2FsjbDqBDGHcbyMetc7k1ctVQD" target="_blank" rel="nofollow">Report (Chinese)</a>
						| <a href="https://wyqdatabase.s3-us-west-1.amazonaws.com/D3Dnet.mp4" target="_blank" rel="nofollow">Demo</a>
						| <a href="https://github.com/XinyiYing/D3Dnet" target="_blank" rel="nofollow">Code</a> 
						<iframe src="https://ghbtns.com/github-btn.html?user=XinyiYing&repo=D3Dnet&type=star&count=true&size=small" 
							frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
						
					</div>
				</div>
			</div>
		
		
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/SMSR.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Learning Sparse Masks for Efficient Image Super-Resolution
					</div>
					<div class="paperName">
						Longguang Wang, Xiaoyu Dong, <b>Yingqian Wang</b>, Xinyi Ying, Zaiping Lin, Wei An, Yulan Guo
					</div>
					<div class="paperPub">
						</span> arXiv, 2020.<br> 
						<span style="color:blue">Locate and skip redundant computation while maintaining comparable SR performance.</span><br>
						| <a href="https://arxiv.org/pdf/2006.09603.pdf" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://github.com/LongguangWang/SMSR" target="_blank" rel="nofollow">Code</a>
					<iframe src="https://ghbtns.com/github-btn.html?user=LongguangWang&repo=SMSR&type=star&count=true&size=small"
							frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
					</div>
				</div>
			</div>
			
			<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/ArbSR.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Learning for Scale-Arbitrary Super-Resolution from Scale-Specific Networks
					</div>
					<div class="paperName">
						Longguang Wang, <b>Yingqian Wang</b>, Zaiping Lin, Jungang Yang, Wei An, Yulan Guo
					</div>
					<div class="paperPub">
						</span> arXiv, 2020.<br> 
						<span style="color:blue">A plug-in module to extend SISR networks for scale-arbitrary (non-integer and asymmetric) SR.</span><br>
						| <a href="https://arxiv.org/pdf/2004.03791.pdf" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://github.com/LongguangWang/Scale-Arbitrary-SR" target="_blank" rel="nofollow">Code</a> 
						<iframe src="https://ghbtns.com/github-btn.html?user=LongguangWang&repo=Scale-Arbitrary-SR&type=star&count=true&size=small"
							frameborder="0" scrolling="0" width="120px" height="20px"></iframe>						
					</div>
				</div>
			</div>
			
			

			<div class="paper short">
				<div class="sub-left">
					<span></span>					
					<img border="0" width="200" height="130" src="imgs/SAM.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						A Stereo Attention Module for Stereo Image Super-Resolution
					</div>
					<div class="paperName">
						Xinyi Ying*, <b>Yingqian Wang*</b>, Longguang Wang, Weidong Sheng, Wei An, Yulan Guo 
					</div>
					<div class="paperPub">
						IEEE Signal Processing Letters, vol. 27, pp. 496-500, 2020. (* co-first authors) <br>
						<span style="color:blue">A generic module to extend arbitrary SISR networks for stereo image SR.</span><br>					
						| <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8998204" target="_blank" rel="nofollow">Paper</a>						
						| <a href="https://mp.weixin.qq.com/s/TyCsUMyoya86wJRiCIXtKA" target="_blank" rel="nofollow">Report (Chinese)</a>						 
						| <a href="https://github.com/XinyiYing/SAM" target="_blank" rel="nofollow">Code</a> 
						<iframe src="https://ghbtns.com/github-btn.html?user=XinyiYing&repo=SAM&type=star&count=true&size=small"
							frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
					</div>
				</div>
			</div>
			
			<div class="paper short">
				<div class="sub-left">
					<span></span>					
					<img border="0" width="200" height="130" src="imgs/DeOccNet.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						DeOccNet: Learning to See Through Foreground Occlusions in Light Fields
					</div>
					<div class="paperName">
						<b>Yingqian Wang</b>, Tianhao Wu, Jungang Yang, Longguang Wang, Wei An, Yulan Guo
					</div>
					<div class="paperPub">
						IEEE Winter Conference on Applications of Computer Vision (WACV), 2020.<br>
						<span style="color:blue">The first deep learning approach for light field de-occlusion.</span><br>	
						| <a href="https://arxiv.org/pdf/1912.04459.pdf" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://mp.weixin.qq.com/s/0K_NF84wvPJttEARVUGPWA" target="_blank" rel="nofollow">Report (Chinese)</a>	
						| <a href="https://yingqianwang.github.io/DeOccNet/Poster.pdf" target="_blank" rel="nofollow">Poster</a>
						| <a href="https://youtu.be/vqHprbEFFis" target="_blank" rel="nofollow">Video Presentation</a>
						| <a href="https://github.com/YingqianWang/DeOccNet" target="_blank" rel="nofollow">Code&Dataset</a> 
						<iframe src="https://ghbtns.com/github-btn.html?user=YingqianWang&repo=DeOccNet&type=star&count=true&size=small"
							frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
					
					</div>
				</div>
			</div>			


	<div class="clear"></div>
	<div class="section">
	<span class="Title"><b>Publications --- 2019</b></span><p><p>
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>					
					<img border="0" width="200" height="120" src="imgs/PASSRnet.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Learning Parallax Attention for Stereo Image Super-Resolution
					</div>
					<div class="paperName">
						Longguang Wang, <b>Yingqian Wang</b>, Zhengfa Liang, Zaiping Lin, Jungang Yang, Wei An, Yulan Guo
					</div>
					<div class="paperPub">
						IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2019.<br>
						<span style="color:blue">Extend attention mechanism to stereo images for super-resolution.</span><br>							
						| <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Learning_Parallax_Attention_for_Stereo_Image_Super-Resolution_CVPR_2019_paper.pdf" target="_blank" rel="nofollow">Paper</a>						
						| <a href="https://yingqianwang.github.io/Flickr1024" target="_blank" rel="nofollow">Dataset</a>
						| <a href="https://mp.weixin.qq.com/s/zN11cI3dOOp1PDXaCPcRng" target="_blank" rel="nofollow">Report (Chinese)</a>
						| <a href="https://github.com/LongguangWang/PASSRnet" target="_blank" rel="nofollow">Code</a> 
						<iframe src="https://ghbtns.com/github-btn.html?user=LongguangWang&repo=PASSRnet&type=star&count=true&size=small" 
							frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
					
					</div>
				</div>
			</div>
		
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>					
					<img border="0" width="200" height="130" src="imgs/Flickr1024.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Flickr1024: A Large-Scale Dataset for Stereo Image Super-Resolution
					</div>
					<div class="paperName">
						<b>Yingqian Wang</b>, Longguang Wang, Jungang Yang, Wei An, Yulan Guo
					</div>
					<div class="paperPub">
						ICCV Workshops, 2019.<br>
						<span style="color:blue">Contains 1024 high-quality stereo images and covers diverse scenarios.</span><br>	
						| <a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/LCI/Wang_Flickr1024_A_Large-Scale_Dataset_for_Stereo_Image_Super-Resolution_ICCVW_2019_paper.pdf" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://yingqianwang.github.io/Flickr1024" target="_blank" rel="nofollow">Dataset</a> 
						<iframe src="https://ghbtns.com/github-btn.html?user=YingqianWang&repo=Flickr1024&type=star&count=true&size=small"
							frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
				
					</div>
				</div>
			</div>
		
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>					
					<img border="0" width="200" height="130" src="imgs/SPL2019.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Selective Light Field Refocusing for Camera Arrays Using Bokeh Rendering and Superresolution
					</div>
					<div class="paperName">
						<b>Yingqian Wang</b>, Jungang Yang, Yulan Guo, Chao Xiao, Wei An
					</div>
					<div class="paperPub">
						IEEE Signal Processing Letters, vol. 26, no. 1, pp. 204-208, 2019. <br>						
						| <a href="https://yingqianwang.github.io/Selective-LF-Refocusing/wang2018selective.pdf" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://github.com/YingqianWang/Selective-LF-Refocusing" target="_blank" rel="nofollow">Code</a> 
						<iframe src="https://ghbtns.com/github-btn.html?user=YingqianWang&repo=Selective-LF-Refocusing&type=star&count=true&size=small"
							frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
				
					</div>
				</div>
			</div>					
		

			<div class="section">
				<span class="Title"><b>Teaching Assistance</b></span><p>			
				<div class="paperPub"><b>
					Lecture: Signals and Systems (Spring Term, 2020)<br>
					Lecture: Optical Detection (Autumn Term, 2019)<br>
					Lecture: Optical Detection (Autumn Term, 2018)<br>
				</b></div>
			</div>
		
		
			<div class="section">
				<span class="Title"><b>Reviewer Service</b></span><p>			
				<div class="paperPub"><b>					
					International Conference on Pattern Recognition (ICPR), 2020 <br>					
					IET Image Processing<br>
					IET Computer Vision<br>
					IEEE Access<br>			
				</b></div>
			</div>
			

			<div class="section">
				<span class="Title"><b>Awards & Honors</b></span><p>			
				<div class="paperPub"><b>
					2018 | Guanghua Scholarship<br>
					2016 | Excellent Graduates of Shandong Province<br>
					2015 | National Scholarship (Ministry of Education, Top 2%)<br>
					2015 | The 1st Prize in the Final of China Mathematics Competitions (45 winners over 63K participants, Top 0.07%)<br>
					2015 | The 1st Prize in China Mathematics Competitions<br>					
					2014 | National Scholarship (Ministry of Education, Top 2%)<br>
					2014 | The 1st Prize in China Mathematics Competitions<br>
					2013 | National Scholarship (Ministry of Education, Top 2%)<br>
					2013 | The 1st Prize in China Mathematics Competitions<br>
				</b></div>
			</div>

	</div>
	
</body>
</html>
