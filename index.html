<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
	<style>
		body{
			color: #333;
		}
		#container{
			min-width: 1100px;
			width: 1100px;
/*			overflow: auto;
*/			margin: 50px auto;padding: 30px;
			/*zoom: 1;*/
*//*			border: 1px solid #ccc;background: #fc9;color: #fff;
*/		}
		#left{
			float: left;
			width: 400px;
			height: 230px;
			margin-left: 0px;
		}
		#right{
			float: left;
			width: auto;
			margin-left: 50px;
		}
		#name{
			font-size: 22.0pt;
		    mso-bidi-font-size: 24.0pt;
		    font-family: Times;
		    mso-bidi-font-family: Times;
		        font-weight: bold;
		}
		#info{
		    font-size: 16.0pt;
		    mso-bidi-font-size: 16.0pt;
		    font-family: Times;
		    mso-bidi-font-family: Times;
		    margin-top: 30px;
		    margin-left: 5px;
		    margin-bottom: 10px;
		    
		}
		.clear{clear:both; height: 0; line-height: 0; font-size: 0}
		.Bio{
			font-size:16.0pt;
			mso-bidi-font-size:17.0pt;
			line-height:150%;
			font-family:Times;
			mso-bidi-font-family:Lato-Regular;
			text-align: justify;
		}
		span.SpellE {
		    mso-style-name: "";
		    mso-spl-e: yes;
		}
		span.Title{
			    font-size: 22.0pt;
			    mso-bidi-font-size: 17.0pt;
			    font-family: Times;
			    mso-bidi-font-family: Lato-Regular;
			    font: bold;
			    margin-top: 10px;
		}
		div.section{
			padding-top: 30px;
		}
		
		div.sub-left{
			float: left;
			width: 250px;
						
		}
		div.sub-left img{
			vertical-align: middle;
			horizontal-align: middle;
			margin-top: 10px;
		}
		
		div.sub-left span{
			height: 100%;
			display: inline-block;
			vertical-align: top;
						
		}
		div.sub-right{
			float: left;
			width: 700px;			
		}
		.paper{
			overflow: auto;
			zoom:1;
			border-top: 2px ;
			padding-bottom: 0px;
			min-height: 160px;

		}
		.paperTitle{
			font-size:16.0pt;
			mso-bidi-font-size:16.0pt;
			font-family:Calibri;
			mso-bidi-font-family:Calibri;
			margin-top: 10px;
			margin-bottom: 10px;
			font-weight: bold;
		}
		.paperName{
		    font-size: 12.5pt;
		    mso-bidi-font-size: 12.5pt;		    
		    font-family: Calibri;
		    mso-bidi-font-family: Times;
		    line-height:160%;
		    font-style: italic;
		}		
		.paperPub{
		    font-size: 15.0pt;
		    mso-bidi-font-size: 14.0pt;		    
		    font-family: Calibri;
		    mso-bidi-font-family: Times;		    
		    font-style: italic;
		    line-height:160%;
		}
		.paperLink{
		    font-size: 14.0pt;
		    mso-bidi-font-size: 14.0pt;		    
		    font-family: Calibri;
		    mso-bidi-font-family: Times;
		    line-height:160%;
		}
		.special{
		    margin-top: 0in;
		    margin-bottom: 0in;
		    margin-left: -.9pt;
		    margin-bottom: .0001pt;
		    text-indent: .9pt;
		    mso-pagination: none;
		    tab-stops: 13.75in;
		    mso-layout-grid-align: none;
		    text-autospace: none;
		}
		.long div.sub-left, .long div.sub-right{
			height: 300px;
			width: 950px;

		}
		.short div.sub-left, .short div.sub-right{
			height:150px;

		}
		div.sub-left,div.sub-right{
			height:200px;

		}
	</style>
</head>
<body>
	<div id="container">
		<div id="left">			
			<img width="400" height="230" src="imgs/photo2.jpg">
		</div>
		<div id="right">
			<div id="name">Yingqian Wang 王应谦 </div>
			<div id="info">

				Ph.D. Student<p>
				National University of Defense Technology (NUDT), China<p>
				Email: wangyingqian16@nudt.edu.cn<p>				
			</div>

			         <a href="https://www.researchgate.net/profile/Yingqian_Wang3?ev=prf_highl" target="_blank" rel="nofollow"><span>Research Gate</span></a>  |
			         <a href="https://github.com/YingqianWang" target="_blank" rel="nofollow"><span>Github</span></a>  |
				 <a href="https://scholar.google.com/citations?user=tBA4alMAAAAJ&hl=en" target="_blank" rel="nofollow"><span>Google Scholar</span></a>
			
			</div>

		<div class="clear"></div>
		<div class="section">
			<span class="Title"><b>Brief Bio</b></span><p>			
				<div class="Bio">
					I received my B.E. degree from Shandong University in 2016, and received my M.E. degree from NUDT in 2018.
					Currently, I'm working toward the Ph.D. degree with the College of Electronic Science and Technology, NUDT.
					My research interests mainly focus on low-level vision, particularly on
					<b style="mso-bidi-font-weight:normal">light field imaging</b> and <b style="mso-bidi-font-weight:normal">image super-resolution</b>.</span></p>
				</div>


	<div class="section">
		<span class="Title"><b>News</b></span><p>			
		<div class="paper long"><b>
			<div class="sub-right">
			<div class="paperName"><b>
			2021.11 | Our paper "Non-Convex Tensor Low-Rank Approximation for Infrared Small Target Detection" is accepted by TGRS. [<a href="https://arxiv.org/abs/2105.14974" target="_blank" rel="nofollow">pdf</a>]<br>	
			2021.11 | Our paper "Detecting and Tracking Small and Dense Moving Objects in Satellite Videos: A Benchmark" is accepted by TGRS.<br>	
			2021.10 | Our paper "DARDet: A Dense Anchor-free Rotated Object Detector in Aerial Images" is accepted by GRSL. [<a href="https://arxiv.org/pdf/2110.01025.pdf" target="_blank" rel="nofollow">pdf</a>] [<a href="https://github.com/zf020114/DARDet" target="_blank" rel="nofollow">code</a>]<br>
			2021.10 | Our paper "Dense Dual-Attention Network for Light Field Image Super-Resolution" is accepted by TCSVT. [<a href="https://arxiv.org/pdf/2110.12114.pdf" target="_blank" rel="nofollow">pdf</a>]<br>				
			2021.10 | Our paper "Arbitrary-Oriented Ship Detection through Center-Head Point Extraction" is accepted by TGRS. [<a href="https://arxiv.org/pdf/2101.11189.pdf" target="_blank" rel="nofollow">pdf</a>] [<a href="https://github.com/zf020114/CHPDet" target="_blank" rel="nofollow">code</a>]<br>	
			2021.10 | Our paper "Spatial-Angular Attention Network for Light Field Reconstruction" is accepted by <span style="color:red">TIP</span>. <br>	
			2021.09 | Our invited Chinese survey on stereo image SR is accepted by <a href="http://www.opticsjournal.net/M/Articles/FullText/lop/58/18/1811014.cshtml" target="_blank" rel="nofollow">Laser & Optoelectronics Progress</a>.<br>
			2021.09 | One paper on SR-based remote sensing ship detection is accepted by GRSL. [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9537901" target="_blank" rel="nofollow">pdf</a>] <br>
			2021.07 | Our paper "Learning a Single Network for Scale-Arbitrary Super-Resolution" is accepted to <span style="color:red">ICCV 2021</span>.<br>
			2021.06 | One paper on remote sensing image classification is accepted by TGRS. [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9495118" target="_blank" rel="nofollow">pdf</a>]<br> 
			2021.04 | Our paper "Symmetric Parallax Attention for Stereo Image Super-Resolution" is accepted as an oral paper to NTIRE 2021.<br>
			2021.03 | Our paper "Unsupervised Degradation Representation Learning for Blind Super-Resolution" is accepted to <span style="color:red">CVPR 2021</span>.<br>
			2021.03 | Our paper "Exploring Sparsity in Image Super-Resolution for Efficient Inference" is accepted to <span style="color:red">CVPR 2021</span>.<br>
			2020.11 | Our paper "Light Field Image Super-Resolution Using Deformable Convolution" is accepted by <span style="color:red">TIP</span>.<br>
			2020.09 | An online tutorial (120 min in Chinese) regarding our Parallax Attention Mechanism is available <a href="https://www.shenlanxueyuan.com/open/course/77" target="_blank" rel="nofollow">here</a>.<br>
			2020.09 | Our paper "Parallax Attention for Unsupervised Stereo Correspondence Learning" is accepted by <span style="color:red">TPAMI</span>.<br>
			2020.07 | Our paper "Spatial-Angular Interaction for Light Field Image Super-Resolution" is accepted to <span style="color:red">ECCV 2020</span>.<br>
			2020.04 | We have collected and released some useful recources on <a href="https://github.com/YingqianWang/Awesome-Stereo-Image-SR" target="_blank" rel="nofollow">Stereo Image SR</a>
	and <a href="https://github.com/YingqianWang/Awesome-LF-Image-SR" target="_blank" rel="nofollow">LF Image SR</a>.<br>
			2019.12 | Our paper "DeOccNet: Learning to See Through Foreground Occlusions in Light Fields" is accepted to WACV 2020.<br>
			2019.03 | A large-scale dataset for stereo image super-resolution is available online at <a href="https://yingqianwang.github.io/Flickr1024" target="_blank" rel="nofollow">Flickr1024</a>. <br>
			2019.02 | Our paper "Learning Parallax Attention for Stereo Image Super-Resolution" is accepted to <span style="color:red">CVPR 2019</span>.<br><br>
			</b></div>
			</b></div>
		</b></div>
	</div>
	
	
	<div class="clear"></div>
	<div class="section">
	<span class="Title"><b>Preprint</b></span><p><p>

		<div class="paper short">
			<div class="sub-left">
				<span></span>
				<img border="0" width="200" height="130" src="imgs/LFT_attmap.png">
			</div>
			<div class="sub-right">
				<div class="paperTitle">
					Light Field Image Super-Resolution with Transformers
				</div>
				<div class="paperName">
					Zhengyu Liang*, <b>Yingqian Wang*</b>, Longguang Wang, Jungang Yang, Shilin Zhou
				</div>
				<div class="paperPub">
					arXiv, 2021.  (* co-first authors) <br>
				</div>
				<div class="paperLink">
					| <a href="https://arxiv.org/pdf/2108.07597.pdf" target="_blank" rel="nofollow">Paper</a>						
					| <a href="https://github.com/ZhengyuLiang24/LFT" target="_blank" rel="nofollow">Code</a>
					<iframe src="https://ghbtns.com/github-btn.html?user=ZhengyuLiang24&repo=LFT&type=star&count=true&size=small"
						frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
				</div>
			</div>
		</div>
		
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/2021_DNANet.png">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Dense Nested Attention Network for Infrared Small Target Detection
					</div>
					<div class="paperName">
						Boyang Li, Chao Xiao, Longguang Wang, <b>Yingqian Wang</b>, Zaiping Lin, Miao Li, Yulan Guo
					</div>
					<div class="paperPub">
						Undergoing a major revision of IEEE TIP.<br>
					</div>
					<div class="paperLink">
						| <a href="https://arxiv.org/pdf/2106.00487.pdf" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://github.com/YeRen123455/Infrared-Small-Target-Detection" target="_blank" rel="nofollow">Code</a>
						<iframe src="https://ghbtns.com/github-btn.html?user=YeRen123455&repo=Infrared-Small-Target-Detection&type=star&count=true&size=small"
							frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
					</div>
				</div>
			</div>
	
		
		
	
	
	<div class="clear"></div>
	<div class="section">
	<span class="Title"><b>Selected Publications --- 2021</b></span><p><p>
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img src="https://longguangwang.github.io/imgs/ArbSR.gif" width="200" height="130">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Learning a Single Network for Scale-Arbitrary Super-Resolution
					</div>
					<div class="paperName">
						Longguang Wang, <b>Yingqian Wang</b>, Zaiping Lin, Jungang Yang, Wei An, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>ICCV</b> </span>, 2021<br> 
					</div>
					<div class="paperLink">
						| <a href="https://arxiv.org/pdf/2004.03791.pdf" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://longguangwang.github.io/Project/ArbSR/" target="_blank" rel="nofollow">Project Page</a>
						| <a href="https://replicate.ai/longguangwang/arbsr" target="_blank" rel="nofollow">Online Demo</a>
						| <a href="https://mp.weixin.qq.com/s/rDtxbt3OPN1wrSe406mVRg" target="_blank" rel="nofollow">News</a>
						| <a href="https://github.com/LongguangWang/Scale-Arbitrary-SR" target="_blank" rel="nofollow">Code</a> 
						<iframe src="https://ghbtns.com/github-btn.html?user=LongguangWang&repo=ArbSR&type=star&count=true&size=small"
							frameborder="0" scrolling="0" width="120px" height="20px"></iframe>						
					</div>
				</div>
			</div>
		
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/DASR.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Unsupervised Degradation Representation Learning for Blind Super-Resolution
					</div>
					<div class="paperName">
						Longguang Wang, <b>Yingqian Wang</b>, Xiaoyu Dong, Qingyu Xu, Jungang Yang, Wei An, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>CVPR</b> </span>, 2021<br> 
					</div>
					<div class="paperLink">
						| <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Unsupervised_Degradation_Representation_Learning_for_Blind_Super-Resolution_CVPR_2021_paper.pdf" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Wang_Unsupervised_Degradation_Representation_CVPR_2021_supplemental.pdf" target="_blank" rel="nofollow">Supp</a>
						| <a href="https://www.techbeat.net/talk-info?id=537" target="_blank" rel="nofollow">Video Presentation</a>						
						| <a href="https://mp.weixin.qq.com/s/jmaMObrWwyg6j659tGe-Kw" target="_blank" rel="nofollow">News</a>
						| <a href="https://github.com/LongguangWang/DASR" target="_blank" rel="nofollow">Code</a>
						<iframe src="https://ghbtns.com/github-btn.html?user=LongguangWang&repo=DASR&type=star&count=true&size=small"
							frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
					</div>
				</div>
			</div>
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/SMSR.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Exploring Sparsity in Image Super-Resolution for Efficient Inference
					</div>
					<div class="paperName">
						Longguang Wang, Xiaoyu Dong, <b>Yingqian Wang</b>, Xinyi Ying, Zaiping Lin, Wei An, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>CVPR</b> </span>, 2021<br> 
					</div>
					<div class="paperLink">
						| <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Exploring_Sparsity_in_Image_Super-Resolution_for_Efficient_Inference_CVPR_2021_paper.pdf" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Wang_Exploring_Sparsity_in_CVPR_2021_supplemental.pdf" target="_blank" rel="nofollow">Supp</a>
						| <a href="https://mp.weixin.qq.com/s/vIacjZk6UvNCxJIdBWY0og" target="_blank" rel="nofollow">News</a>
						| <a href="https://www.bilibili.com/video/BV1eL411x7sy?spm_id_from=333.999.0.0" target="_blank" rel="nofollow">Video Presentation</a>
						| <a href="https://github.com/LongguangWang/SMSR" target="_blank" rel="nofollow">Code</a>
						<iframe src="https://ghbtns.com/github-btn.html?user=LongguangWang&repo=SMSR&type=star&count=true&size=small"
							frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
					</div>
				</div>
			</div>		


			<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/iPASSR.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Symmetric Parallax Attention for Stereo Image Super-Resolution
					</div>
					<div class="paperName">
						<b>Yingqian Wang*</b>, Xinyi Ying*, Longguang Wang, Jungang Yang, Wei An, Yulan Guo
					</div>
					<div class="paperPub">
						CVPRW, 2021. <br>
					</div>
					<div class="paperLink">																	
						| <a href="https://arxiv.org/pdf/2011.03802.pdf" target="_blank" rel="nofollow">Paper</a>						
						| <a href="https://wyqdatabase.s3-us-west-1.amazonaws.com/iPASSR_visual_comparison.mp4" target="_blank" rel="nofollow">Demo</a>
						| <a href="https://wyqdatabase.s3.us-west-1.amazonaws.com/Submission_0021_video_presentation.mp4" target="_blank" rel="nofollow">Presentation</a>
						| <a href="https://github.com/YingqianWang/iPASSR" target="_blank" rel="nofollow">Code</a>
						<iframe src="https://ghbtns.com/github-btn.html?user=YingqianWang&repo=iPASSR&type=star&count=true&size=small"
							frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
					</div>
				</div>
			</div>		
			

			<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/SAAN.png">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Spatial-Angular Attention Network for Light Field Reconstruction
					</div>
					<div class="paperName">
						Gaochang Wu, <b>Yingqian Wang</b>, Yebin Liu, Lu Fang, Tianyou Chai
					</div>
					<div class="paperPub">
						IEEE TIP, 2021.<br>
					</div>
					<div class="paperLink">
						| <a href="https://arxiv.org/pdf/2007.02252v2.pdf" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://gaochangwu.github.io/SAAN/SAA-Net.html" target="_blank" rel="nofollow">Project Page</a>						
						| <a href="https://github.com/GaochangWu/SAAN" target="_blank" rel="nofollow">Code</a>
						<iframe src="https://ghbtns.com/github-btn.html?user=GaochangWu&repo=SAAN&type=star&count=true&size=small"
							frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
					</div>
				</div>
			</div>

			<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/DDAN.png">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Dense Dual-Attention Network for Light Field Image Super-Resolution
					</div>
					<div class="paperName">
						Yu Mo, <b>Yingqian Wang</b>, Chao Xiao, Jungang Yang, Wei An
					</div>
					<div class="paperPub">
						IEEE TCSVT, 2021.<br>
					</div>
					<div class="paperLink">
						| <a href="https://arxiv.org/pdf/2110.12114.pdf" target="_blank" rel="nofollow">Paper</a>						
					</div>
				</div>
			</div>
		

			
	
	
	<div class="clear"></div>
	<div class="section">
	<span class="Title"><b>Selected Publications --- 2020</b></span><p><p>


                <div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/PASMnet.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Parallax Attention for Unsupervised Stereo Correspondence Learning
					</div>
					<div class="paperName">
						Longguang Wang, Yulan Guo, <b>Yingqian Wang</b>, Zhengfa Liang, Zaiping Lin, Jungang Yang, Wei An
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>IEEE TPAMI</b> </span>, 2021<br>
					</div>
					<div class="paperLink">
						| <a href="https://arxiv.org/pdf/2009.08250.pdf" target="_blank" rel="nofollow">Paper</a>						
						| <a href="https://www.shenlanxueyuan.com/open/course/77" target="_blank" rel="nofollow">Tutorial</a>
						| <a href="https://mp.weixin.qq.com/s/EdEwOEm5ttj3IM-6jI_s4A" target="_blank" rel="nofollow">News</a>						
						| <a href="https://yingqianwang.github.io/Flickr1024" target="_blank" rel="nofollow">Dataset</a>
						| <a href="https://github.com/LongguangWang/PAM" target="_blank" rel="nofollow">Code</a>
						<iframe src="https://ghbtns.com/github-btn.html?user=LongguangWang&repo=PAM&type=star&count=true&size=small"
							frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
						
					</div>
				</div>
			</div>	
		
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/SFEAFE.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Spatial-Angular Interaction for Light Field Image Super-Resolution
					</div>
					<div class="paperName">
						<b>Yingqian Wang</b>, Longguang Wang, Jungang Yang, Wei An, Jingyi Yu, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>ECCV</b> </span>, 2020<br>
					</div>
					<div class="paperLink">
						| <a href="https://arxiv.org/pdf/1912.07849.pdf" target="_blank" rel="nofollow">Paper</a>						
						| <a href="https://mp.weixin.qq.com/s?__biz=MzIwMTE1NjQxMQ==&mid=2247520164&idx=1&sn=beb7031162b5c173e66a77b01a828e5b&chksm=96f0fdf0a18774e6a9f85e316efd64d9990dca2213e353c2da373b0a6f4004717e9b092923fb&mpshare=1&srcid=07057miMA6psG3L6UHJZOYb1&sharer_sharetime=1593964266602&sharer_shareid=eadfa6ebb7f5bf94747b471d67269b5e&from=timeline&scene=2&subscene=1&clicktime=1593965476&enterid=1593965476&ascene=14&devicetype=Windows+10+x64&version=62090529&nettype=3gnet&abtest_cookie=AAACAA%3D%3D&lang=zh_CN&exportkey=AbY3Hk%2Bj7%2FZr1bhOoseHkrg%3D&pass_ticket=xxF82PuauoRDV3pp3gMzbUWag4Wn9ibURKETDRhWZzgqvAuhhLZpHUAsxPEkeaTP&wx_header=1&key=d197f1f59dba99c28d4c1b99b04b6bd1077e51e74b63c15b1c47bf892712247f8c43b777c67399686f47ac2805132c5cecb7dd7d8fcd8ede07661f7a2dc7733f6be335fc77d4b5033daad98428a3d3ef&uin=MTM2MDA1MjgzOA%3D%3D" target="_blank" rel="nofollow">News</a>
						| <a href="https://wyqdatabase.s3-us-west-1.amazonaws.com/LF-InterNet.mp4" target="_blank" rel="nofollow">Presentation</a>
						| <a href="https://github.com/YingqianWang/LF-InterNet" target="_blank" rel="nofollow">Code</a>
						<iframe src="https://ghbtns.com/github-btn.html?user=YingqianWang&repo=LF-InterNet&type=star&count=true&size=small"
							frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
					</div>
				</div>
			</div>
		
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/LF-DFnet.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Light Field Image Super-Resolution Using Deformable Convolution
					</div>
					<div class="paperName">
						<b>Yingqian Wang</b>, Jungang Yang, Longguang Wang, Xinyi Ying, Tianhao Wu, Wei An, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>IEEE TIP</b> </span>, 2020<br>
					</div>
					<div class="paperLink">
						| <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9286855" target="_blank" rel="nofollow">Paper</a>						
						| <a href="https://github.com/YingqianWang/LF-DFnet" target="_blank" rel="nofollow">Code</a>
						<iframe src="https://ghbtns.com/github-btn.html?user=YingqianWang&repo=LF-DFnet&type=star&count=true&size=small"
							frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
					</div>
				</div>
			</div>
		
		
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/D3Dnet.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Deformable 3D Convolution for Video Super-Resolution
					</div>
					<div class="paperName">
						Xinyi Ying, Longguang Wang, <b>Yingqian Wang</b>, Weidong Sheng, Wei An, Yulan Guo
					</div>
					<div class="paperPub">
						IEEE SPL, 2020.<br> 
					</div>
					<div class="paperLink">
						| <a href="https://arxiv.org/pdf/2004.02803.pdf" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247502648&idx=2&sn=17e7e6b7aa13fc6cd39a2104de094919&chksm=ec1c26c1db6bafd7d62cbf50f8530bd035bbcac90994de9802370cddfec24fd451fa012c8e85&mpshare=1&scene=1&srcid=0730qMSCTUETD2peq7Ai9mqj&sharer_sharetime=1596065203111&sharer_shareid=989b0bb833dbad1aaaaf36960593e33d&key=e15af338f75799eee85c83cac60bac15ac1e15e525ccfd752c3e15a9507a29048c2a839916476162b7cd6631f963b446002008fb9da77ec44e2d36984d0c673fa2f3cbb751941bd96ca9c48496b49e0c&ascene=1&uin=MTM2MDA1MjgzOA%3D%3D&devicetype=Windows+10+x64&version=62090529&lang=zh_CN&exportkey=AVTx8nleIvzQ0rnSOvP%2Ftzg%3D&pass_ticket=%2F9wOS6SK9msDx7isZOq2Wr5mNTqm2vAhUeKvKok%2FsjbDqBDGHcbyMetc7k1ctVQD" target="_blank" rel="nofollow">News</a>
						| <a href="https://wyqdatabase.s3-us-west-1.amazonaws.com/D3Dnet.mp4" target="_blank" rel="nofollow">Demo</a>
						| <a href="https://github.com/XinyiYing/D3Dnet" target="_blank" rel="nofollow">Code</a>						
						<iframe src="https://ghbtns.com/github-btn.html?user=XinyiYing&repo=D3Dnet&type=star&count=true&size=small"
							frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
						
					</div>
				</div>
			</div>	
		
			

			<div class="paper short">
				<div class="sub-left">
					<span></span>					
					<img border="0" width="200" height="130" src="imgs/SAM.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						A Stereo Attention Module for Stereo Image Super-Resolution
					</div>
					<div class="paperName">
						Xinyi Ying*, <b>Yingqian Wang*</b>, Longguang Wang, Weidong Sheng, Wei An, Yulan Guo 
					</div>
					<div class="paperPub">
						IEEE SPL, 2020. (* co-first authors) <br>
					</div>
					<div class="paperLink">
						| <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8998204" target="_blank" rel="nofollow">Paper</a>						
						| <a href="https://github.com/XinyiYing/SAM" target="_blank" rel="nofollow">Code</a> 
						<iframe src="https://ghbtns.com/github-btn.html?user=XinyiYing&repo=SAM&type=star&count=true&size=small"
							frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
					</div>
				</div>
			</div>
			
			<div class="paper short">
				<div class="sub-left">
					<span></span>					
					<img border="0" width="200" height="130" src="imgs/DeOccNet.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						DeOccNet: Learning to See Through Foreground Occlusions in Light Fields
					</div>
					<div class="paperName">
						<b>Yingqian Wang</b>, Tianhao Wu, Jungang Yang, Longguang Wang, Wei An, Yulan Guo
					</div>
					<div class="paperPub">
						WACV, 2020.<br>
					</div>
					<div class="paperLink">
						| <a href="https://arxiv.org/pdf/1912.04459.pdf" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://mp.weixin.qq.com/s/0K_NF84wvPJttEARVUGPWA" target="_blank" rel="nofollow">News</a>	
						| <a href="https://yingqianwang.github.io/DeOccNet/Poster.pdf" target="_blank" rel="nofollow">Poster</a>
						| <a href="https://github.com/YingqianWang/DeOccNet" target="_blank" rel="nofollow">Code&Dataset</a> 
						<iframe src="https://ghbtns.com/github-btn.html?user=YingqianWang&repo=DeOccNet&type=star&count=true&size=small"
							frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
					
					</div>
				</div>
			</div>			


	<div class="clear"></div>
	<div class="section">
	<span class="Title"><b>Selected Publications --- 2019</b></span><p><p>
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>					
					<img border="0" width="200" height="120" src="imgs/PASSRnet.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Learning Parallax Attention for Stereo Image Super-Resolution
					</div>
					<div class="paperName">
						Longguang Wang, <b>Yingqian Wang</b>, Zhengfa Liang, Zaiping Lin, Jungang Yang, Wei An, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>CVPR</b> </span>, 2019<br> 
					</div>
					<div class="paperLink">
						| <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Learning_Parallax_Attention_for_Stereo_Image_Super-Resolution_CVPR_2019_paper.pdf" target="_blank" rel="nofollow">Paper</a>						
						| <a href="https://yingqianwang.github.io/Flickr1024" target="_blank" rel="nofollow">Dataset</a>
						| <a href="https://mp.weixin.qq.com/s/zN11cI3dOOp1PDXaCPcRng" target="_blank" rel="nofollow">News</a>
						| <a href="https://github.com/LongguangWang/PASSRnet" target="_blank" rel="nofollow">Code</a> 
						<iframe src="https://ghbtns.com/github-btn.html?user=LongguangWang&repo=PASSRnet&type=star&count=true&size=small" 
							frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
					
					</div>
				</div>
			</div>
		
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>					
					<img border="0" width="200" height="130" src="imgs/Flickr1024.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Flickr1024: A Large-Scale Dataset for Stereo Image Super-Resolution
					</div>
					<div class="paperName">
						<b>Yingqian Wang</b>, Longguang Wang, Jungang Yang, Wei An, Yulan Guo
					</div>
					<div class="paperPub">
						ICCVW, 2019.<br>
					</div>
					<div class="paperLink">
						| <a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/LCI/Wang_Flickr1024_A_Large-Scale_Dataset_for_Stereo_Image_Super-Resolution_ICCVW_2019_paper.pdf" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://yingqianwang.github.io/Flickr1024" target="_blank" rel="nofollow">Dataset</a> 
						<iframe src="https://ghbtns.com/github-btn.html?user=YingqianWang&repo=Flickr1024&type=star&count=true&size=small"
							frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
				
					</div>
				</div>
			</div>
		
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>					
					<img border="0" width="200" height="130" src="imgs/SPL2019.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Selective Light Field Refocusing for Camera Arrays Using Bokeh Rendering and Superresolution
					</div>
					<div class="paperName">
						<b>Yingqian Wang</b>, Jungang Yang, Yulan Guo, Chao Xiao, Wei An
					</div>
					<div class="paperPub">
						IEEE SPL, 2019. <br>
					</div>
					<div class="paperLink">
						| <a href="https://arxiv.org/pdf/2108.03918.pdf" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://github.com/YingqianWang/Selective-LF-Refocusing" target="_blank" rel="nofollow">Code</a> 
						<iframe src="https://ghbtns.com/github-btn.html?user=YingqianWang&repo=Selective-LF-Refocusing&type=star&count=true&size=small"
							frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
				
					</div>
				</div>
			</div>					
		

			<div class="section">
				<span class="Title"><b>Teaching Assistance</b></span><p>			
				<div class="paperName"><b>
					Lecture: Optical Imaging and Detection (Spring Term, 2021)<br>
					Lecture: Optical Imaging and Detection (Autumn Term, 2020)<br>
					Lecture: Signals and Systems (Spring Term, 2020)<br>
					Lecture: Target Detection and Signal Processing (Autumn Term, 2019)<br>
					Lecture: Target Detection and Signal Processing (Autumn Term, 2018)<br>
				</b></div>
			</div>
		
		
			<div class="section">
				<span class="Title"><b>Conference Reviewer</b></span><p>			
				<div class="paperName"><b>
					IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022 <br>
					International Conference on Computer Vision (ICCV), 2021 <br>					
					ACM International Conference on Multimedia (ACM MM), 2021 <br>
					New Trends in Image Restoration and Enhancement Workshop (CVPRW) <br>
					IEEE International Conference on Multimedia and Expo (ICME) <br>
					International Conference on Pattern Recognition (ICPR) <br>
					International Conference on Image and Graphics (ICIG) <br>
					CAAI International Conference on Artificial Intelligence (CICAI) <br>					
				</b></div>
			</div>
		
			<div class="section">
				<span class="Title"><b>Journal Reviewer</b></span><p>			
				<div class="paperName"><b>					
					IEEE Transactions on Image Processing (IEEE T-IP) <br>
					IEEE Transactions on Circuits and Systems for Video Technology (IEEE T-CSVT) <br>
					IEEE Geoscience and Remote Sensing Letters (IEEE GRSL) <br>
					Elsevier Knowledge-Based Systems <br>
					IET Computer Vision <br>
					IET Image Processing <br>					
					IEEE Access <br>
				</b></div>
			</div>
		
					

			<div class="section">
				<span class="Title"><b>Awards & Honors</b></span><p>			
				<div class="paperName"><b>
					2021 | Outstanding Master Dissertation Award of Hunan Province<br>
					2018 | Guanghua Scholarship<br>
					2016 | Excellent Graduates of Shandong Province<br>
					2015 | National Scholarship (Ministry of Education, Top 2%)<br>
					2015 | The 1st Prize in the Final of China Mathematics Competitions (45 winners over 63K participants, Top 0.07%)<br>
					2015 | The 1st Prize in China Mathematics Competitions<br>					
					2014 | National Scholarship (Ministry of Education, Top 2%)<br>
					2014 | The 1st Prize in China Mathematics Competitions<br>
					2013 | National Scholarship (Ministry of Education, Top 2%)<br>
					2013 | The 1st Prize in China Mathematics Competitions<br>
				</b></div>
			</div>		
		
			<!-- site visitors begjin -->
			<div style="margin:50px 0;">
				<a href="https://clustrmaps.com/site/1bffo" title="Visit tracker"><img src="//clustrmaps.com/map_v2.png?cl=ffffff&w=500&t=tt&d=ueKrfCS3qabq9AqNETgGVXDkjNud6pEFK3nRS1f1NxQ" /></a>
			</div>
			<!-- site visitors end -->
		
			<!-- Last update time begjin -->
			<div style="border-top: 3px solid #555; text-align: center;">
				<p style="color: #555;">Last updated: 2021-12-07</p>
			</div>
			<!-- Last update time end -->

	</div>
	
</body>
</html>
